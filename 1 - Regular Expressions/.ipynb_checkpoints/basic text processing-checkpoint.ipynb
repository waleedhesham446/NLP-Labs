{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\omars\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\omars\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\omars\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer,LancasterStemmer,WordNetLemmatizer\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction to Regular Expressions for NLP\n",
    "\n",
    "### What are Regular Expressions (Regex)?\n",
    "\n",
    " Regular expressions (regex or regexp) are powerful tools for pattern matching and text manipulation. They are a sequence of characters that define a search pattern. Regex allows you to search, match, and manipulate strings based on specific patterns or rules.\n",
    "\n",
    "### Why Use Regular Expressions in NLP?\n",
    "\n",
    "In Natural Language Processing, regular expressions are useful for:\n",
    "\n",
    "- Text cleaning and preprocessing.\n",
    "- Extracting specific information from text.\n",
    "- Validating and parsing text data.\n",
    "- Pattern matching for entity recognition, tokenization, and more.\n",
    "\n",
    "### Basic Syntax\n",
    "\n",
    "Regex patterns are constructed using a combination of regular characters and metacharacters.\n",
    "\n",
    "- Regular characters (e.g., letters, numbers) are matched literally.\n",
    "- Metacharacters (e.g., . ^ $ * + ? { } [ ] | ( ) \\) have special meanings.\n",
    "\n",
    "### Python `re` Module\n",
    "\n",
    " Python's `re` module provides functions and classes for working with regular expressions.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's start by importing the `re` module:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing re module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" regulare expression are compiled into pattern objects, which have methods for various operations\n",
    "such as searching for pattern matches or performing string substitutions. \"\"\"\n",
    "\n",
    "p = re.compile('ab*')\n",
    "\n",
    "\"\"\" re.compile also accepts optional flags to control the behavior of the pattern.\n",
    "For example, to make the matching case-insensitive, we can pass in the flag re.IGNORECASE or re.IGNORECASE:\"\"\"\n",
    "\n",
    "p = re.compile('ab*', re.I)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Flag               | Description                                   |\n",
    "|--------------------|-----------------------------------------------|\n",
    "| re.IGNORECASE or re.I | Ignores a case.                             |\n",
    "| re.DOTALL or re.S  | Allows the . metch any character to match a newline.|\n",
    "| re.MULTILINE or re.M | Allows the ^ and $ metacharacters to match each line.|\n",
    "| re.VERBOSE or re.X  | Allows whitespaces and comments in pattern compilation.|\n",
    "| re.ASCII or re.A     | Makes \\w, \\W, \\b, \\B, \\s, \\S match only ASCII characters.|\n",
    "| re.LOCALE or re.L     | Makes \\w, \\W, \\b, \\B, \\s, \\S match according to the current locale.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Example of using verbose flag to make the pattern more readable \"\"\"\n",
    "pattern = re.compile(r\"\"\"\\d +  # the integral part\n",
    "                   \\.    # the decimal point\n",
    "                   \\d *  # some fractional digits\"\"\", re.X)\n",
    "pattern = re.compile(r\"\\d+\\.\\d*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in regular expressions we use \\ to escape special characters\n",
    "for example \\d is a special character that matches any digit\n",
    "so if we want to match a \\d we need to escape it with another \\\n",
    "so we need to write \\\\\\d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(6, 8), match='\\\\d'>\n"
     ]
    }
   ],
   "source": [
    "pattern= re.compile(r\"\\\\d\")\n",
    "print(pattern.search(\"abc123\\def\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to match a backslash character? \n",
    "In regular expressions all backslashes and metacharacters must be escaped with a backslash so the regular expression that matches a \\ is “\\\\\\\\”\n",
    "\n",
    "To pass that in a regular python string object each backslash needs to be escaped also by backslashes so we need to pass “\\\\\\\\\\\\\\\\” to match only one backslash.\n",
    "\n",
    "To solve this problem, we can use python raw string notation for regular expressions where the backslashes are not handled in special way. Python raw string are prefixed with r → r”\\\\\\\\” matches just single backslash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(6, 7), match='\\\\'>\n",
      "<re.Match object; span=(6, 7), match='\\\\'>\n"
     ]
    }
   ],
   "source": [
    "pattern= re.compile(\"\\\\\\\\\")\n",
    "print(pattern.search(\"abc123\\def\"))\n",
    "\n",
    "pattern= re.compile(r\"\\\\\")\n",
    "print(pattern.search(\"abc123\\def\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preforming matches\n",
    "\n",
    "the pattern object has several function to preform match, the table cover most important ones\n",
    "\n",
    "| Method               | Purpose                                   |\n",
    "|--------------------|-----------------------------------------------|\n",
    "| match() | Determine if the RE matches at the beginning of the string.|\n",
    "| search()  | Scan through a string, looking for any location where this RE matches.|\n",
    "| findall() | Find all substrings where the RE matches, and returns them as a list.|\n",
    "| finditer() | Find all substrings where the RE matches, and returns them as an iterator.|\n",
    "| sub() | Find all substrings where the RE matches, and replace them with a different string.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 1: matching for a Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match: <re.Match object; span=(0, 3), match='abb'>\n",
      "Match found: abb\n",
      "Starting index: 0\n",
      "Ending index: 3\n",
      "Number of characters matched: 3\n",
      "Matched text: abbaaabbbbaaaaa\n"
     ]
    }
   ],
   "source": [
    "# match for pattern\n",
    "text = \"abbaaabbbbaaaaa\"\n",
    "# use the battern object to match the text\n",
    "match=p.match(text)\n",
    "if match:\n",
    "    # match object has many attributes and methods\n",
    "    print(\"Match:\", match)\n",
    "    print(\"Match found:\", match.group())\n",
    "    print(\"Starting index:\", match.start())\n",
    "    print(\"Ending index:\", match.end())\n",
    "    print(\"Number of characters matched:\", match.end() - match.start())\n",
    "    print(\"Matched text:\", match.string)\n",
    "    \n",
    "else:\n",
    "    print(\"No match found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 2: Searching for a Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match found: $2\n",
      "Starting index: 28\n",
      "Ending index: 30\n",
      "Number of characters matched: 2\n",
      "Matched text: The price of the product is $2 $25.99.\n"
     ]
    }
   ],
   "source": [
    "# Search for a pattern in text\n",
    "text = \"The price of the product is $2 $25.99.\"\n",
    "pattern = r'\\$\\d+(\\.\\d{2})?'\n",
    "# Note: You can use the top-level functions provided by re without creating  pattern objects\n",
    "match = re.search(pattern, text) # match is none if no match is found\n",
    "if match:\n",
    "    # match object has many attributes and methods\n",
    "    print(\"Match found:\", match.group())\n",
    "    print(\"Starting index:\", match.start())\n",
    "    print(\"Ending index:\", match.end())\n",
    "    print(\"Number of characters matched:\", match.end() - match.start())\n",
    "    print(\"Matched text:\", match.string)\n",
    "    \n",
    "else:\n",
    "    print(\"No match found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 3: Extracting Email Addresses with findall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email addresses found: ['john@example.com', 'mary@example.org']\n"
     ]
    }
   ],
   "source": [
    "# Extract email addresses from text\n",
    "text = \"Contact us at john@example.com or mary@example.org.\"\n",
    "pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b'\n",
    "matches = re.findall(pattern, text)\n",
    "print(\"Email addresses found:\", matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 4: Extracting Email Addresses with finditer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match found: john@example.com\n",
      "Starting index: 14\n",
      "Ending index: 30\n",
      "Number of characters matched: 16\n",
      "Matched text: Contact us at john@example.com or mary@example.org.\n",
      "-----------------------------\n",
      "Match found: mary@example.org\n",
      "Starting index: 34\n",
      "Ending index: 50\n",
      "Number of characters matched: 16\n",
      "Matched text: Contact us at john@example.com or mary@example.org.\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "# Extract email addresses from text\n",
    "text = \"Contact us at john@example.com or mary@example.org.\"\n",
    "pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b'\n",
    "matches = re.finditer(pattern, text)\n",
    "for match in matches:\n",
    "    print(\"Match found:\", match.group())\n",
    "    print(\"Starting index:\", match.start())\n",
    "    print(\"Ending index:\", match.end())\n",
    "    print(\"Number of characters matched:\", match.end() - match.start())\n",
    "    print(\"Matched text:\", match.string)\n",
    "    print(\"-----------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 5: Text Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned text: This is an example sentence with pecial character\n"
     ]
    }
   ],
   "source": [
    "# Clean text by removing non-alphanumeric characters\n",
    "text = \"This is an example sentence with $pecial character$.\"\n",
    "cleaned_text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "print(\"Cleaned text:\", cleaned_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Stemming      | Lemmatization |\n",
    "|---------------|--------------|\n",
    "| A process that stems or removes the last few characters from a word, often resulting in incorrect meanings and spelling.  | Considers the context and converts a word to its meaningful base form, known as the lemma.\n",
    "| For example, stemming 'Caring' would return 'Car'.| For example, lemmatizing 'Caring' would return 'Care'. |\n",
    "| Stemming is faster. | Lemmatization is slower. |\n",
    "| looking at the form of the word. | looking at the meaning of the word. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK includes several off-the-shelf stemmers, and if you ever need a stemmer you should use one of these in preference to crafting your own using regular expressions, since these handle a wide range of irregular cases.\n",
    "\n",
    "Two popular stemmers in NLTK:\n",
    "- Porter stemmer\n",
    "- Lancaster stemmer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|                    | PorterStemmer                         | LancasterStemmer                   |\n",
    "|--------------------|--------------------------------------|------------------------------------|\n",
    "| Use Cases          | Useful when you want to preserve word meanings to some extent. | Suitable when you want a more aggressive stemming to reduce words to their base form.|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Word--            --Stem--            \n",
      "program             program             \n",
      "programming         program             \n",
      "programer           program             \n",
      "programs            program             \n",
      "programmed          program             \n",
      "happy               happi               \n",
      "happily             happili             \n",
      "Happier             happier             \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n--Word--            --Stem--            \\nprogram             program             \\nprogramming         program             \\nprogramer           program             \\nprograms            program             \\nprogrammed          program\\n\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Initialize Python porter stemmer\n",
    "ps = PorterStemmer()\n",
    "# Example inflections to reduce\n",
    "example_words = [\"program\",\"programming\",\"programer\",\"programs\",\"programmed\", \"happy\", \"happily\",\"Happier\"]\n",
    "# Perform stemming\n",
    "print(\"{0:20}{1:20}\".format(\"--Word--\",\"--Stem--\"))\n",
    "for word in example_words:\n",
    "   print (\"{0:20}{1:20}\".format(word, ps.stem(word)))\n",
    "\n",
    "\"\"\"\n",
    "--Word--            --Stem--            \n",
    "program             program             \n",
    "programming         program             \n",
    "programer           program             \n",
    "programs            program             \n",
    "programmed          program\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Word--            --Stem--            \n",
      "program             program             \n",
      "programming         program             \n",
      "programer           program             \n",
      "programs            program             \n",
      "programmed          program             \n",
      "happy               happy               \n",
      "happily             happy               \n",
      "Happier             happy               \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n--Word--            --Stem--            \\nprogram             program             \\nprogramming         program             \\nprogramer           program             \\nprograms            program             \\nprogrammed          program\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Initialize Python porter stemmer\n",
    "ps = LancasterStemmer()\n",
    "# Example inflections to reduce\n",
    "example_words = [\"program\",\"programming\",\"programer\",\"programs\",\"programmed\", \"happy\", \"happily\",\"Happier\"]\n",
    "# Perform stemming\n",
    "print(\"{0:20}{1:20}\".format(\"--Word--\",\"--Stem--\"))\n",
    "for word in example_words:\n",
    "   print (\"{0:20}{1:20}\".format(word, ps.stem(word)))\n",
    "\n",
    "\"\"\"\n",
    "--Word--            --Stem--            \n",
    "program             program             \n",
    "programming         program             \n",
    "programer           program             \n",
    "programs            program             \n",
    "programmed          program\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Word--            --Lemma--           \n",
      "program             program             \n",
      "programming         program             \n",
      "programer           programer           \n",
      "programs            program             \n",
      "programmed          program             \n",
      "happy               happy               \n",
      "happily             happily             \n",
      "Happier             Happier             \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n--Word--            --Lemma--           \\nprogram             program             \\nprogramming         program             \\nprogramer           programer           \\nprograms            program             \\nprogrammed          program\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize wordnet lemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "# Example inflections to reduce\n",
    "example_words = [\"program\",\"programming\",\"programer\",\"programs\",\"programmed\", \"happy\", \"happily\", \"Happier\"]\n",
    "# Perform lemmatization\n",
    "print(\"{0:20}{1:20}\".format(\"--Word--\",\"--Lemma--\"))\n",
    "for word in example_words:\n",
    "   print (\"{0:20}{1:20}\".format(word, wnl.lemmatize(word, pos=\"v\")))\n",
    "\"\"\"\n",
    "--Word--            --Lemma--           \n",
    "program             program             \n",
    "programming         program             \n",
    "programer           programer           \n",
    "programs            program             \n",
    "programmed          program\n",
    "\"\"\"   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is the task of cutting a string into identifiable linguistic units that constitute a piece of language data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split on : ['This', 'is', 'an', 'example', 'sentence.', '', '', '', '', 'We', 'will', 'split', 'this', 'into', 'words.']\n",
      "split on space/tabs/newlines : ['This', 'is', 'an', 'example', 'sentence.', 'We', 'will', 'split', 'this', 'into', 'words.']\n",
      "split on spaces : ['This', 'is', 'an', 'example', 'sentence.', 'We', 'will', 'split', 'this', 'into', 'words.']\n"
     ]
    }
   ],
   "source": [
    "#The simplest approach is to split text on white space\n",
    "text = \"This is an example sentence.     We will split this into words.\"\n",
    "\n",
    "words=re.split(r' ', text)\n",
    "print(\"split on :\", words)\n",
    "\n",
    "# What about tabs and newlines?  What about double, triple white spaces?\n",
    "\n",
    "words=re.split(r'[ \\t\\n]+', text)\n",
    "print(\"split on space/tabs/newlines :\", words)\n",
    "#Or\n",
    "words=re.split(r'\\s+', text)\n",
    "print(\"split on spaces :\", words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized text: ['This', 'is', 'an', 'example', 'sentence', '.', 'We', 'will', 'split', 'this', 'into', 'words', '.']\n"
     ]
    }
   ],
   "source": [
    "# tokenize text into words\n",
    "words = nltk.word_tokenize(text)\n",
    "print(\"Tokenized text:\", words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPE tokenization (Byte Pair Encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "def byte_pair_tokenizer(corpus, vocab_size=1000, min_frequency=2):\n",
    "    # Initialize a tokenizer\n",
    "    tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "    # Train the tokenizer on the corpus\n",
    "    tokenizer.train_from_iterator(corpus, vocab_size=vocab_size, min_frequency=min_frequency)\n",
    "\n",
    "    # Return the tokenizer\n",
    "    return tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Tokenization</w>': 2,\n",
       " 'is</w>': 2,\n",
       " 'the</w>': 3,\n",
       " 'process</w>': 1,\n",
       " 'of</w>': 2,\n",
       " 'breaking</w>': 1,\n",
       " 'down</w>': 1,\n",
       " 'a</w>': 1,\n",
       " 'sequence</w>': 1,\n",
       " 'text</w>': 2,\n",
       " 'into</w>': 2,\n",
       " 'smaller</w>': 1,\n",
       " 'units</w>': 1,\n",
       " 'called</w>': 1,\n",
       " 'tokens,</w>': 1,\n",
       " 'which</w>': 1,\n",
       " 'can</w>': 1,\n",
       " 'be</w>': 1,\n",
       " 'words,</w>': 1,\n",
       " 'phrases,</w>': 1,\n",
       " 'or</w>': 1,\n",
       " 'even</w>': 1,\n",
       " 'individual</w>': 1,\n",
       " 'characters</w>': 1,\n",
       " 'often</w>': 1,\n",
       " 'first</w>': 1,\n",
       " 'step</w>': 1,\n",
       " 'in</w>': 1,\n",
       " 'natural</w>': 1,\n",
       " 'languages</w>': 1,\n",
       " 'processing</w>': 2,\n",
       " 'tasks</w>': 1,\n",
       " 'such</w>': 2,\n",
       " 'as</w>': 3,\n",
       " 'classification,</w>': 1,\n",
       " 'named</w>': 1,\n",
       " 'entity</w>': 1,\n",
       " 'recognition,</w>': 1,\n",
       " 'and</w>': 1,\n",
       " 'sentiment</w>': 1,\n",
       " 'analysis</w>': 1,\n",
       " 'The</w>': 1,\n",
       " 'resulting</w>': 1,\n",
       " 'tokens</w>': 2,\n",
       " 'are</w>': 2,\n",
       " 'typically</w>': 1,\n",
       " 'used</w>': 1,\n",
       " 'input</w>': 1,\n",
       " 'to</w>': 2,\n",
       " 'further</w>': 1,\n",
       " 'steps,</w>': 1,\n",
       " 'vectorization,</w>': 1,\n",
       " 'where</w>': 1,\n",
       " 'converted</w>': 1,\n",
       " 'numerical</w>': 1,\n",
       " 'representations</w>': 1,\n",
       " 'for</w>': 1,\n",
       " 'machine</w>': 1,\n",
       " 'learning</w>': 1,\n",
       " 'models</w>': 1,\n",
       " 'use</w>': 1}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re \n",
    "from collections import defaultdict \n",
    "\n",
    "def get_vocab(data): \n",
    "\t\"\"\" \n",
    "\tGiven a list of strings, returns a dictionary of words mapping to their frequency \n",
    "\tcount in the data. \n",
    "\n",
    "\tParameters:\n",
    "\tdata: list of strings\n",
    "\n",
    "\tReturns:\n",
    "\tdictionary of words mapping to their frequency count in the data\n",
    "\n",
    "\tExample:\n",
    "\tIf the input 'data' is ['low', 'lower'], the function returns:\n",
    "\t{'l o w </w>': 1, 'l o w e r </w>': 1}\n",
    "\t\"\"\"\n",
    "\tvocab = defaultdict(int) \n",
    "\tfor line in data: \n",
    "\t\tfor word in line.split(): \n",
    "\t\t\tvocab[' '.join(list(word)) + ' </w>'] += 1\n",
    "\treturn vocab \n",
    "\n",
    "\n",
    "def get_stats(vocab): \n",
    "\t\"\"\"\n",
    "\tGiven a vocabulary (a dictionary mapping words to frequency counts), this function returns a dictionary of tuples representing the frequency count of pairs of characters in the vocabulary.\n",
    "\n",
    "\tParameters:\n",
    "\t\tvocab (dict[str, int]): A dictionary where keys are words, and values are their frequency counts.\n",
    "\n",
    "\tReturns:\n",
    "\t\tdict[tuple[str, str], int]: A dictionary where keys are tuples of two characters, and values are the frequency count of those character pairs.\n",
    "\n",
    "\tExample:\n",
    "\t\tIf the input 'vocab' is {'l o w </w>': 5, 'l o w e r </w>': 2}, the function returns:\n",
    "\t\t{('l', 'o'): 7, ('o', 'w'): 7, ('w', '</w>'): 5, ('w', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2}\n",
    "\t\"\"\"\n",
    "\tpairs = defaultdict(int) \n",
    "\tfor word, freq in vocab.items(): \n",
    "\t\tsymbols = word.split() \n",
    "\t\tfor i in range(len(symbols)-1): \n",
    "\t\t\tpairs[symbols[i],symbols[i+1]] += freq \n",
    "\treturn pairs \n",
    "\n",
    "def merge_vocab(pair, v_in): \n",
    "\t\"\"\" \n",
    "\tGiven a pair of characters and a vocabulary, returns a new vocabulary with the \n",
    "\tpair of characters merged together wherever they appear. \n",
    "\n",
    "\tParameters:\n",
    "\t\tpair: tuple of two characters\n",
    "\t\tv_in: dictionary of words mapping to their frequency count in the data\n",
    "\t\n",
    "\tReturns:\n",
    "\t\tV_out: dictionary of words mapping to their frequency count in the data with the pair merged\n",
    "\n",
    "\tExample:\n",
    "\t\tIf the input 'pair' is ('e', 'r') and the input 'v_in' is {'l o w </w>': 5, 'l o w e r </w>': 2},\n",
    "\t\tthe function returns:\n",
    "\t\t{'l o w </w>': 5, 'l o w er </w>': 2}\n",
    "\n",
    "\n",
    "\t\"\"\"\n",
    "\tv_out = {} \n",
    "\tbigram = re.escape(' '.join(pair)) \n",
    "\t#  Negative Lookbehind (?<!\\S) => it matches a position where the character before it is a whitespace character or the beginning of the string\n",
    "\t#  Negative Lookahead (?!\\S)  => it matches a position where the character after it is a whitespace character or the end of the string\n",
    "\t# \\S => non white space [^\\s]\n",
    "\tp = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)') \n",
    "\tfor word in v_in: \n",
    "\t\tw_out = p.sub(''.join(pair), word) \n",
    "\t\tv_out[w_out] = v_in[word] \n",
    "\treturn v_out \n",
    "\n",
    "\n",
    "def byte_pair_encoding(data, n): \n",
    "\t\"\"\" \n",
    "\tGiven a list of strings and an integer n, returns a list of n merged pairs \n",
    "\tof characters found in the vocabulary of the input data. \n",
    "\t\"\"\"\n",
    "\tvocab = get_vocab(data) \n",
    "\tfor i in range(n): \n",
    "\t\tpairs = get_stats(vocab) \n",
    "\t\tbest = max(pairs, key=pairs.get) \n",
    "\t\tvocab = merge_vocab(best, vocab) \n",
    "\treturn vocab \n",
    "\n",
    "# Example usage: \n",
    "corpus = '''Tokenization is the process of breaking down \n",
    "a sequence of text into smaller units called tokens, \n",
    "which can be words, phrases, or even individual characters. \n",
    "Tokenization is often the first step in natural languages processing tasks \n",
    "such as text classification, named entity recognition, and sentiment analysis. \n",
    "The resulting tokens are typically used as input to further processing steps, \n",
    "such as vectorization, where the tokens are converted \n",
    "into numerical representations for machine learning models to use.'''\n",
    "data = corpus.split('.') \n",
    "\n",
    "n = 230\n",
    "bpe_pairs = byte_pair_encoding(data, n) \n",
    "bpe_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "1. https://www.geeksforgeeks.org/byte-pair-encoding-bpe-in-nlp/\n",
    "2. https://docs.python.org/3/library/re.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
